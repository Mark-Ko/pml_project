#Human Activity Recognition for Sports Training  
#####Created by Mark K. on 16 February 2015  
  

###Introduction 

  In the last ten years there has been a proliferation of relatively inexpensive wearable sensors such as Jawbone Up, Nike FuelBand, and Fitbit. In Human Activity Recognition (HAR) wearable sensors are used to determine patterns in human activity. For example, with wearable sensors, HAR can monitor daily activity for medical diagnosis and rehabilitation, and encourage people to adopt a healthy lifestyle. [Tessendorf, et. al., 2011](http://www.researchgate.net/profile/Bert_Arnrich/publication/235834654_An_IMU-based_Sensor_Network_to_Continuously_Monitor_Rowing_Technique_on_the_Water/links/02bfe513f51a0d82d2000000.pdf) and [Velloso, et. al., 2013]( https://www.andreas-bulling.de/fileadmin/docs/velloso13_ah.pdf)  have described a new paradigm, not just monitoring the quantity of activity, but the quality of the activity. This is directly applicable to sports, where "quality recognition" could be used to supply feedback on performance and technique to the athlete or coach.  
  
[Velloso, et. al., 2013]( https://www.andreas-bulling.de/fileadmin/docs/velloso13_ah.pdf)  looked at qualitative activity recognition in a weight Lifting task, the single arm dumbbell biceps curl. They placed sensors on a glove and upper arm band worn on the lifting arm. Sensors were also placed on a lumbar lifting belt and one head of the dumbbell.  Six subjects performed sets of bicep curls under five different conditions:  
Class A - normal "gold standard" lift  
Class B - throwing the elbows to the front  
Class C - lifting the weight only half way  
Class D - lowering the weight only half way  
Class E - throwing the hips forward  

Classes B through E simulated poor form in performing the lift. The authors demonstrated that they could detect a small number of mistakes in technique using wearable sensors and standard pattern recognition techniques.
  
    
Velloso, et. al., 2013 made their sensor data [publically available](http://groupware.les.inf.puc-rio.br/har) along with the class of the lift performed (their variable "classe".) I used this data to develop a model that classifies the biceps curl based on the sensor data. First I cleaned and explored the data. Then I tested three models. Finally I used my best model to predict the class of test cases.
  
```{r prelims, message=FALSE, warning=FALSE, echo=TRUE}
options(warn = -1)  
library(knitr)
library(caret)
library(randomForest)
library(corrplot)
```
   
###Load the Data   
  
The training data for this project are available here:   
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  
  
 
```{r load, cache=TRUE, results='hide'}
urlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("./data/pml-training.csv")) {
  download.file(urlTrain, destfile="./data/pml-training.csv")
}
if (!file.exists("./data/pml-testing.csv")) {
  download.file(urlTest, destfile="./data/pml-testing.csv")
}
HARtrain <- read.csv("./data/pml-training.csv", na.strings = c("NA","#DIV/0!", ""))
HARtest <- read.csv("./data/pml-testing.csv", na.strings = c("NA","#DIV/0!", ""))
```
  
  
The citation for this data is: Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.     


###Explore and Clean the Data  
  
There are 19622 records with 160 variables in the training data set and 20 records with 160 variables in the testing data set.  
  
The data contained many columns with missing data and NAs, in addition to some records containing #DIV/0!. When downloading, I set these values equal to NA. I created a reduced data set by removing columns with NA values. I further reduced the data set by removing the first seven columns, which contained subject identifiers and time-stamp information. This left me with 53 variables: 52 sensor variables and the classification variable, *classe*.
  
```{r clean, cache=TRUE, results='hide'}
dim(HARtrain)
dim(HARtest)
# find columns of null values
varsHARtrain <- rownames(na.omit(data.frame(t(HARtrain))))
varsHARtest <- rownames(na.omit(data.frame(t(HARtest))))
HARtrain <- HARtrain[,varsHARtrain]
HARtest <- HARtest[, varsHARtest]
# remove subject id and time stamps
colnames(HARtrain[,1:7])
HARtrain <- HARtrain[, -c(1:7)]
HARtest <- HARtest[, -c(1:7)]
dim(HARtrain)
dim(HARtest)
```
   
   
###Preprocess the Data  
  
I partitioned the HAR training data set into a training and testing set so I could perform cross validation. I split the data set  70/30. The training set had 13737 records and the testing set had 5885 records. I found that there were no variables with near zero variance.  
  
```{r preproces, cache=TRUE, results='hide'}
set.seed(5564)
inTrain <- createDataPartition(y=HARtrain$classe,
                               p=0.7, list=FALSE)
training <- HARtrain[inTrain,]
testing <- HARtrain[-inTrain,]
dim(training)
dim(testing)
near0 <- nearZeroVar(training)
near0
```
  

```{r distributionclasse, cache=TRUE}
combo <- as.data.frame(rbind(summary(training$classe), summary(testing$classe)))
rownames(combo) <- c("training", "testing")
kable(combo, align = c("c","c","c","c","c"), 
      caption = "Table 1 Composition of Data Sets by Biceps Curl Class")
```
  
There are approximately 50% more records for the ideal performance (Class A) in my training and testing data sets, Table 1.
    
###Develop My Model  
  
The arm, trunk, and dumbbell can be considered a linked system of rigid bodies during the dumbbell curl. In a biomechanical system there will be coupled motion due to the constraints of the links. I would expect some of the motions at sensors were coupled. Thus, the features may not all be independent and some may be highly correlated. Figure 1 is a correlation plot of the features. The sensors on the lumbar belt are correlated. There is also some correlation between the  gyros on the forearm and dumbbell.  Below I list the features with high correlation (>0.85).  

```{r correlation, fig.height=8, fig.width=8, dpi=100}
corrplot(cor(training[,-53]), tl.cex = 0.8, mar = c(0,0,1,0),
         title = "Figure 1 Correlation Plot of Sensor Variables")
par(mar = c(5.1, 4.1, 4.1, 2.1))
hicorr <- findCorrelation(cor(training[,-53]), cutoff = 0.85)
colnames(training[hicorr])
```  
 
  
####Naive Bayes  
  
I first tried a naive Bayes model with "nb" in caret. I used 7-fold cross validation. The accuracy on the test set was 0.7395 (**out of sample error of 26.05%**.) This wasn't accurate enough.  

  
```{r nb, message=FALSE, warning=FALSE}
modnb <- train(classe ~ ., method = "nb", data = training, 
                 trControl=trainControl(method="cv", number=7))
pnb <- predict(modnb, testing)
confusionMatrix(pnb, testing$classe)$table
confusionMatrix(pnb, testing$classe)$overall[1:2]
```
  
####Gradient Boosting with Trees  
  
For my next model I tried gradient boosting with "gbm" in caret. I used 7-fold cross validation. The accuracy on the test set was impressive, 0.9664 (**out of sample error of 3.36%**.) This model was quite an improvement over naive Bayes.
  
```{r gbm, message=FALSE, warning=FALSE}
modgbm <- train(classe ~ ., method = "gbm", data = training, 
                 trControl=trainControl(method="cv", number=7), verbose = FALSE)
pgbm <- predict(modgbm, testing)
print(modgbm$finalModel)
confusionMatrix(pgbm, testing$classe)$table
confusionMatrix(pgbm, testing$classe)$overall[1:2]
```

  
####Random Forest  
  
For my last model I decided to try a random forest. During "play" I found that using the function randomForest() was much faster than using the caret train() wrapper.  [Random forest doesn't require cross-validation]( http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr). The out-of-bag (OOB) error is an unbiased estimate of the classification error as trees are added to the forest. I decided to cross-validate with the testing set out of fear (irrational perhaps!) of over-fitting. The OOB classification error of my model was 0.53%. **The out of sample error was 0.42%**; showing that cross-validation really wasn't needed.   
  
I list the 20 most important features in my "black box" random forest model and plot them in Figure 2. It's surprising that three of the top five features were sensors on the belt, not on the forearm or dumbbell. The error rates for the different classes, reached their minimums after approximately 100 trees, Figure 3. The smallest errors were for the ideal biceps curl (Class A) and the curl with hips forward (Class E). An overlaid density plot for the top 8 features shows that there doesn't appear to be simple separability of classes, except perhaps for pitch forearm and roll forearm, Figure 4. 

  
```{r randomForest}
set.seed(5564)
modrf <- randomForest(classe ~., data = training)
modrf
prf <- predict(modrf, newdata=testing)
confusionMatrix(prf, testing$classe)
vimprf <- data.frame(rownames(varImp(modrf)), varImp(modrf)[,1])
colnames(vimprf) <- c("variable", "Overall")
index <- order(vimprf$Overall, decreasing = TRUE)
vimprf <- vimprf[index,]
rownames(vimprf) <- c(1:length(vimprf$variable))
vimprf[1:20,]
```
 
    
```{r final plots, fig.height=6, fig.width=8, dpi=100}
options(digits = 2)
options(scipen=999)
varImpPlot(modrf, n.var=  20, 
           main = "Figure 2 Variable Importance for Random Forest Model")
par(mar = c(5, 4, 4, 4))
plot(modrf, log = "y", axes = TRUE, frame.plot = TRUE, main = "Figure 3 Random Forest Model Error Rates")
legend("topright", colnames(modrf$err.rate), col = 1:6, fill = 1:6)

featurePlot(x = testing[,index[1:8]], y = testing$classe, plot = "density", 
            main = "Figure 4 Density Plots For Random Forest Model",
            adjust = 1.5, pch = "|", layout = c(4,2), 
            scales=list(x=list(relation="free"), y=list(relation="free")),
            auto.key = list(columns = 3))
```
  
  

###Conclusion  
  
I was able to predict the *quality* of a dumbbell curl. My random forest model was surprisingly accurate, 99.58% on the cross-validation testing set. Naive Bayes was disappointing. For the top features, the probability density functions of the classes were similar and didn't appear easily separable. Also, based on the biomechanics of the system, I would expect some sensor variables to be coupled, thus dependent, a violation of the "naive" assumption. This may explain the poor performance of the naive Bayes classifier.  


###Addendum - The Test Cases  
  
My random forest model correctly classified all 20 test cases.  

  
```{r test, results='hide'}
answers <- predict(modrf, newdata=HARtest)
answers <- as.character(answers)
```
   

```{r formygrade, results='hide'}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./pml_answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```

